{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": "!pip install pyts",
   "id": "59e749185811e437",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-11-02T10:23:23.401687Z",
     "start_time": "2024-11-02T10:22:35.354528Z"
    }
   },
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import multiprocessing\n",
    "import tensorflow as tf\n",
    "\n",
    "class color:\n",
    "   PURPLE = '\\033[95m'\n",
    "   CYAN = '\\033[96m'\n",
    "   DARKCYAN = '\\033[36m'\n",
    "   BLUE = '\\033[94m'\n",
    "   GREEN = '\\033[92m'\n",
    "   YELLOW = '\\033[93m'\n",
    "   RED = '\\033[91m'\n",
    "   BOLD = '\\033[1m'\n",
    "   UNDERLINE = '\\033[4m'\n",
    "   END = '\\033[0m'\n",
    "    \n",
    "\n",
    "RND_SEED = 0\n",
    "GPU_ID = 0\n",
    "USE_GPU = True\n",
    "\n",
    "if tf.config.list_physical_devices('GPU') and USE_GPU:\n",
    "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "    gpu_name = gpus[0].name  # You may specify an index if using multiple GPUs\n",
    "    print(f\"Using GPU - {gpu_name}\")\n",
    "    device = '/GPU:0'\n",
    "else:\n",
    "    device = '/CPU:0'\n",
    "\n",
    "# To use the device, you can use a context manager in TensorFlow:\n",
    "with tf.device(device):\n",
    "    # Your code that uses the specified device\n",
    "    pass\n",
    "\n",
    "print(f\"Using device {device}\")\n",
    "\n",
    "N_CPU_CORES = multiprocessing.cpu_count()\n",
    "\n",
    "BASE_FOLDER = os.path.join(\n",
    "    \"..\", \"input\", \"tlvmc-parkinsons-freezing-gait-prediction\"\n",
    ")\n",
    "\n",
    "print(f\"Number of CPU cores available: {N_CPU_CORES}\")\n",
    "\n",
    "pd.set_option('display.max_columns', 30)\n",
    "pd.set_option('display.max_rows', 200)\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "import os\n",
    "import numpy as np\n",
    "from tensorflow.keras.utils import Sequence, to_categorical\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU - /physical_device:GPU:0\n",
      "Using device /GPU:0\n",
      "Number of CPU cores available: 12\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-01T18:09:13.940567Z",
     "start_time": "2024-11-01T18:09:13.897192Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class CustomMultiInputDataGenerator(Sequence):\n",
    "    def __init__(self, directories, batch_size=32, image_size=(64, 64), shuffle=True, augment=False, num_classes=2, **kwargs):\n",
    "        \"\"\"\n",
    "        directories: List of directories, one for each input branch.\n",
    "        \"\"\"\n",
    "        super().__init__(**kwargs)\n",
    "        self.directories = directories\n",
    "        self.batch_size = batch_size\n",
    "        self.image_size = image_size\n",
    "        self.shuffle = shuffle\n",
    "        self.augment = augment\n",
    "        self.num_classes = num_classes\n",
    "        self.image_paths = self._load_image_paths()\n",
    "        self.samples = len(self.image_paths[0])  # Assume all branches have the same number of images\n",
    "        \n",
    "        # Initialize the ImageDataGenerator for augmentation\n",
    "        self.datagen = ImageDataGenerator(\n",
    "            rescale=1/255.0,\n",
    "            width_shift_range=0.1 if self.augment else 0,\n",
    "            height_shift_range=0.1 if self.augment else 0\n",
    "        )\n",
    "        \n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def _load_image_paths(self):\n",
    "        # Load image paths for each branch directory\n",
    "        image_paths = []\n",
    "        for directory in self.directories:\n",
    "            branch_image_paths = [os.path.join(directory, fname) for fname in os.listdir(directory) if fname.endswith('.jpg')]\n",
    "            image_paths.append(branch_image_paths)\n",
    "        return image_paths\n",
    "\n",
    "    def _get_class_from_filename(self, filename):\n",
    "        # Extract label from filename assuming a naming convention\n",
    "        class_label = int(filename.split('_')[-1].split('.')[0])\n",
    "        return class_label\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(self.samples / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Load a batch of images for each branch\n",
    "        batch_image_paths = [paths[index * self.batch_size:(index + 1) * self.batch_size] for paths in self.image_paths]\n",
    "        \n",
    "        # Load and preprocess images for each branch\n",
    "        images_per_branch = []\n",
    "        for branch_paths in batch_image_paths:\n",
    "            images = np.array([img_to_array(load_img(path, target_size=self.image_size)) for path in branch_paths])\n",
    "            if self.augment:\n",
    "                images = np.array([self.datagen.random_transform(image) for image in images])\n",
    "            else:\n",
    "                images = self.datagen.standardize(images)\n",
    "            images_per_branch.append(images)\n",
    "\n",
    "        # Load labels (assuming the same labels for each branch)\n",
    "        labels = np.array([self._get_class_from_filename(os.path.basename(path)) for path in batch_image_paths[0]])\n",
    "        labels = to_categorical(labels, num_classes=self.num_classes)\n",
    "        \n",
    "        return images_per_branch, labels\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            for branch_paths in self.image_paths:\n",
    "                np.random.shuffle(branch_paths)\n"
   ],
   "id": "bf0a3b107a4eab55",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-01T18:21:01.569363Z",
     "start_time": "2024-11-01T18:21:01.560073Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from models.cnn_models import create_multi_input_cnn\n",
    "img_shape = (64, 64)"
   ],
   "id": "3b26a3551f1cec59",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "train_directories = [\"data/mean_subtract/gaf_images/AccAP/train\", \"data/mean_subtract/gaf_images/AccML/train\", \"data/mean_subtract/gaf_images/AccV/train\"]\n",
    "\n",
    "val_directories = [\"data/mean_subtract/gaf_images/AccAP/valid\", \"data/mean_subtract/gaf_images/AccML/valid\", \"data/mean_subtract/gaf_images/AccV/valid\"]\n",
    "\n",
    "train_generator = CustomMultiInputDataGenerator(\n",
    "    directories=train_directories,\n",
    "    batch_size=32,\n",
    "    image_size=img_shape,\n",
    "    augment=False,\n",
    "    num_classes=2\n",
    ")\n",
    "\n",
    "val_generator = CustomMultiInputDataGenerator(\n",
    "    directories=val_directories,\n",
    "    batch_size=32,\n",
    "    image_size=img_shape,\n",
    "    augment=False,\n",
    "    num_classes=2\n",
    ")\n",
    "\n",
    "model = create_multi_input_cnn()\n",
    "\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    validation_data=val_generator,\n",
    "    epochs=60\n",
    ")"
   ],
   "id": "32f400e6e0b1f418",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Save Model",
   "id": "c8265645ba42c124"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "import pickle",
   "id": "b6ac8f282d051d26"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "with open('model_history.pkl', 'wb') as f:\n",
    "    pickle.dump(history.history, f)"
   ],
   "id": "68cb0244f302a36b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "with open('model_history.pkl', 'rb') as f:\n",
    "    loaded_history = pickle.load(f)"
   ],
   "id": "dd15d0fcef5d3dbe"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "model.save('multi_input_cnn_model_0_9067_f1score_60_epoch.h5')",
   "id": "72b3770474f20db0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Get Model",
   "id": "9e1d193ecc738c24"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-01T18:19:02.741177Z",
     "start_time": "2024-11-01T18:19:02.381066Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "def f1_metric(y_true, y_pred):\n",
    "    # Convert predictions from softmax to class predictions\n",
    "    y_true = tf.argmax(y_true, axis=-1)\n",
    "    y_pred = tf.argmax(y_pred, axis=-1)\n",
    "\n",
    "    # Cast to float32 for compatibility\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    y_pred = tf.cast(y_pred, tf.float32)\n",
    "\n",
    "    # Calculate metrics\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "\n",
    "    f1_val = 2 * (precision * recall) / (precision + recall + K.epsilon())\n",
    "    return f1_val\n",
    "\n",
    "model = load_model('multi_input_cnn_model_0_9067_f1score_60_epoch.h5', custom_objects={'f1_metric': f1_metric})"
   ],
   "id": "f836532f1903c966",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Plot Performance",
   "id": "1f96c70023240dc0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-01T18:19:06.676669Z",
     "start_time": "2024-11-01T18:19:06.362429Z"
    }
   },
   "cell_type": "code",
   "source": "from matplotlib import pyplot as plt",
   "id": "bceef8e5bf88d16b",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig = plt.figure()\n",
    "plt.plot(history.history['loss'], color='teal', label='loss')\n",
    "plt.plot(history.history['val_loss'], color='orange', label='val_loss')\n",
    "fig.suptitle('Loss', fontsize=20)\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.show()"
   ],
   "id": "6a63ce020dab9ed7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig = plt.figure()\n",
    "plt.plot(history.history['accuracy'], color='teal', label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], color='orange', label='val_accuracy')\n",
    "fig.suptitle('Accuracy', fontsize=20)\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.show()"
   ],
   "id": "47d685e119c74a18",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-01T18:28:45.179111Z",
     "start_time": "2024-11-01T18:21:11.404785Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def evaluate_model(model, generator):\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    \n",
    "    for i in range(len(generator)):\n",
    "        images, batch_y_true = generator[i]\n",
    "        \n",
    "        # Get predictions from model\n",
    "        batch_predictions = model.predict(images, verbose=0)\n",
    "        \n",
    "        # Convert softmax output to binary class predictions\n",
    "        batch_y_pred = np.argmax(batch_predictions, axis=1)\n",
    "        \n",
    "        # Flatten ground truth if it's one-hot encoded\n",
    "        batch_y_true = np.argmax(batch_y_true, axis=1)\n",
    "        \n",
    "        y_true.extend(batch_y_true.flatten())\n",
    "        y_pred.extend(batch_y_pred.flatten())\n",
    "    \n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    \n",
    "    metrics = {\n",
    "        'f1_micro': f1_score(y_true, y_pred, average='micro'),\n",
    "        'f1_macro': f1_score(y_true, y_pred, average='macro'),\n",
    "        'f1_weighted': f1_score(y_true, y_pred, average='weighted'),\n",
    "        'f1_per_class': f1_score(y_true, y_pred, average=None)\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "test_directories = [\"data/mean_subtract/gaf_images/AccAP/test\", \"data/mean_subtract/gaf_images/AccML/test\", \"data/mean_subtract/gaf_images/AccV/test\"]\n",
    "\n",
    "test_generator = CustomMultiInputDataGenerator(\n",
    "    directories=test_directories,\n",
    "    batch_size=32,\n",
    "    image_size=img_shape,\n",
    "    augment=False,\n",
    "    num_classes=2\n",
    ")\n",
    "\n",
    "results = evaluate_model(model, test_generator)\n",
    "\n",
    "print(\"\\nModel Evaluation Results:\")\n",
    "print(f\"Micro F1 Score: {results['f1_micro']:.4f}\")\n",
    "print(f\"Macro F1 Score: {results['f1_macro']:.4f}\")\n",
    "print(f\"Weighted F1 Score: {results['f1_weighted']:.4f}\")\n",
    "print(\"\\nF1 Score per class:\")\n",
    "for i, score in enumerate(results['f1_per_class']):\n",
    "    print(f\"Class {i}: {score:.4f}\")"
   ],
   "id": "1ac57b33811ac630",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Evaluation Results:\n",
      "Micro F1 Score: 0.8759\n",
      "Macro F1 Score: 0.8586\n",
      "Weighted F1 Score: 0.8756\n",
      "\n",
      "F1 Score per class:\n",
      "Class 0: 0.8091\n",
      "Class 1: 0.9081\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-01T19:03:25.699007Z",
     "start_time": "2024-11-01T19:02:38.339825Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "def evaluate_model(model, generator):\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    \n",
    "    # Loop over the generator to collect all predictions and true labels\n",
    "    for i in range(len(generator)):\n",
    "        images, batch_y_true = generator[i]\n",
    "        \n",
    "        # Get predictions from the model\n",
    "        batch_predictions = model.predict(images, verbose=0)\n",
    "        \n",
    "        # Convert softmax output to binary class predictions\n",
    "        batch_y_pred = np.argmax(batch_predictions, axis=1)\n",
    "        \n",
    "        # Convert ground truth if it's one-hot encoded\n",
    "        batch_y_true = np.argmax(batch_y_true, axis=1)\n",
    "        \n",
    "        y_true.extend(batch_y_true.flatten())\n",
    "        y_pred.extend(batch_y_pred.flatten())\n",
    "    \n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    \n",
    "    # Calculate confusion matrix\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    \n",
    "    # Total number of samples\n",
    "    total_samples = len(y_true)\n",
    "    \n",
    "    # Convert counts to percentages\n",
    "    tn_percent = (tn / total_samples) * 100\n",
    "    fp_percent = (fp / total_samples) * 100\n",
    "    fn_percent = (fn / total_samples) * 100\n",
    "    tp_percent = (tp / total_samples) * 100\n",
    "    \n",
    "    # Calculate F1 metrics\n",
    "    metrics = {\n",
    "        'f1_micro': f1_score(y_true, y_pred, average='micro'),\n",
    "        'f1_macro': f1_score(y_true, y_pred, average='macro'),\n",
    "        'f1_weighted': f1_score(y_true, y_pred, average='weighted'),\n",
    "        'f1_per_class': f1_score(y_true, y_pred, average=None),\n",
    "        'true_positives_percentage': tp_percent,\n",
    "        'true_negatives_percentage': tn_percent,\n",
    "        'false_positives_percentage': fp_percent,\n",
    "        'false_negatives_percentage': fn_percent\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "results = evaluate_model(model, test_generator)\n",
    "\n",
    "print(results)"
   ],
   "id": "c481e3059a89dc3f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'f1_micro': 0.8732214228617107, 'f1_macro': 0.855911585371256, 'f1_weighted': 0.8730856907411674, 'f1_per_class': array([0.80597015, 0.90585302]), 'true_positives_percentage': 60.99120703437249, 'true_negatives_percentage': 26.33093525179856, 'false_positives_percentage': 6.474820143884892, 'false_negatives_percentage': 6.203037569944045}\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "results = model.evaluate(test_generator)\n",
    "print(f\"Test Results: {results}\")"
   ],
   "id": "ffa6944adbd35acf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T12:38:28.073313Z",
     "start_time": "2024-10-31T12:38:28.061306Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for name, value in zip(model.metrics_names, results):\n",
    "    print(f\"{name}: {value:.4f}\")"
   ],
   "id": "aef80652a81e745d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.4418\n",
      "accuracy: 0.8723\n",
      "precision: 0.8723\n",
      "recall: 0.8723\n",
      "f1_metric: 0.9031\n"
     ]
    }
   ],
   "execution_count": 42
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Verify there weren't any leakage",
   "id": "5207eb31f5d1bdc9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T10:41:21.281400Z",
     "start_time": "2024-10-31T10:41:21.254261Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_dir = 'data/mean_subtract/gaf_images/AccML/train'\n",
    "test_dir = 'data/mean_subtract/gaf_images/AccML/test'\n",
    "valid_dir = 'data/mean_subtract/gaf_images/AccML/valid'\n",
    "\n",
    "# Get the list of files in each folder\n",
    "train_files = set(os.listdir(train_dir))\n",
    "test_files = set(os.listdir(test_dir))\n",
    "valid_files = set(os.listdir(valid_dir))\n",
    "\n",
    "# Find common files\n",
    "common_files = train_files & test_files & valid_files\n",
    "\n",
    "# Print or list the common files\n",
    "print(\"Common files:\", common_files)"
   ],
   "id": "8eae68cb2853728d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Common files: set()\n"
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T12:51:58.041362Z",
     "start_time": "2024-10-31T12:51:58.025738Z"
    }
   },
   "cell_type": "code",
   "source": "test_files & valid_files",
   "id": "8531a6af8c3ff0b2",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T10:41:52.998745Z",
     "start_time": "2024-10-31T10:41:52.951819Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define the directory path\n",
    "directories = {\n",
    "    \"train\": 'data/mean_subtract/gaf_images/AccAP/train',\n",
    "    \"valid\": 'data/mean_subtract/gaf_images/AccAP/valid',\n",
    "    \"test\": 'data/mean_subtract/gaf_images/AccAP/test',\n",
    "}\n",
    "\n",
    "for type, dir in directories.items():\n",
    "    count_1 = 0\n",
    "    count_0 = 0\n",
    "    print(f\"For {type} ==>\")\n",
    "    for filename in os.listdir(dir):\n",
    "        if filename.endswith('_1.jpg'):\n",
    "            count_1 += 1\n",
    "        elif filename.endswith('_0.jpg'):\n",
    "            count_0 += 1\n",
    "    \n",
    "    # Print the counts\n",
    "    print(f\"class 1: {count_1}\")\n",
    "    print(f\"class 0: {count_0}\")\n"
   ],
   "id": "363d29252f242642",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For train ==>\n",
      "class 1: 3182\n",
      "class 0: 5106\n",
      "For valid ==>\n",
      "class 1: 710\n",
      "class 0: 1700\n",
      "For test ==>\n",
      "class 1: 4203\n",
      "class 0: 2052\n"
     ]
    }
   ],
   "execution_count": 39
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
